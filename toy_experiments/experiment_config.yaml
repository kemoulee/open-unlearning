# NPO Unlearn Toy实验配置文件
defaults:
  - /model: Llama-3.2-1B-Instruct
  - /trainer: NPO
  - /data: unlearn
  - /collator: DataCollatorForSupervisedDataset
  - /hydra: default
  - /paths: default
  - _self_

# 模型配置
model:
  model_args:
    pretrained_model_name_or_path: open-unlearning/tofu_Llama-3.2-1B-Instruct_full
    torch_dtype: bfloat16
    attn_implementation: flash_attention_2
    device_map: auto

# 数据配置
data:
  anchor: forget
  forget:
    TOFU_QA_forget: 
      args:
        hf_args:
          name: forget10
  retain:
    TOFU_QA_retain:
      args:
        hf_args:
          name: retain90

# Trainer配置
trainer:
  handler: NPO
  args:
    warmup_epochs: 1.0 # custom parameter
    learning_rate: 1e-5
    weight_decay: 0.01
    num_train_epochs: 10
    save_strategy: steps
    save_steps: 50
    logging_steps: 50
    remove_unused_columns: false
    dataloader_num_workers: 0
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 1

# 实验参数
forget_split: forget10
retain_split: retain90
task_name: 1B_10_NPO_toy
mode: unlearn 