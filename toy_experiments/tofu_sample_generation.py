#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TOFUæ•°æ®é›†ç¬¬18æ¡æ•°æ®çš„top-Pé‡‡æ ·ç”Ÿæˆè„šæœ¬
é’ˆå¯¹å·²åœ¨TOFUæ•°æ®é›†ä¸Šè®­ç»ƒçš„Llama-3.2-1Bæ¨¡å‹ï¼Œä»forget10%æ•°æ®é›†ä¸­é‡‡æ ·ç¬¬18æ¡æ•°æ®ï¼Œä½¿ç”¨top-P samplingç”Ÿæˆ5æ¡å›å¤
"""

import torch
import os
import sys
import json
from datasets import load_dataset
from omegaconf import DictConfig, OmegaConf
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.pathï¼Œä»¥ä¾¿å¯¼å…¥srcæ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from model import get_model

# è®¾ç½®ç¼“å­˜ç›®å½•ï¼Œä¸é¡¹ç›®æ ‡å‡†åšæ³•ä¸€è‡´
HF_HOME = os.getenv('HF_HOME', os.path.expanduser('~/.cache/huggingface'))

def create_model_config():
    """åˆ›å»ºæ¨¡å‹é…ç½®ï¼Œä½¿ç”¨å·²åœ¨TOFUæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹"""
    model_config = {
        "model_args": {
            "pretrained_model_name_or_path": "open-unlearning/tofu_Llama-3.2-1B-Instruct_full",
            "attn_implementation": "flash_attention_2",
            "torch_dtype": "bfloat16",
            "device_map": "auto"  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
        },
        "tokenizer_args": {
            "pretrained_model_name_or_path": "open-unlearning/tofu_Llama-3.2-1B-Instruct_full"
        },
        "template_args": {
            "apply_chat_template": True,
            "system_prompt": "You are a helpful assistant.",
            "system_prompt_with_special_tokens": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant.<|eot_id|>",
            "user_start_tag": "<|start_header_id|>user<|end_header_id|>\n\n",
            "user_end_tag": "<|eot_id|>",
            "asst_start_tag": "<|start_header_id|>assistant<|end_header_id|>\n\n",
            "asst_end_tag": "<|eot_id|>",
            "date_string": "10 Apr 2025"
        }
    }
    return OmegaConf.create(model_config)

def load_tofu_dataset():
    """åŠ è½½TOFU forget10%æ•°æ®é›†"""
    print("ğŸ“¥ æ­£åœ¨åŠ è½½TOFUæ•°æ®é›†...")
    dataset = load_dataset("locuslab/TOFU", name="forget10", split="train")
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}æ¡æ•°æ®")
    return dataset

def load_trained_model():
    """åŠ è½½å·²åœ¨TOFUæ•°æ®é›†ä¸Šè®­ç»ƒçš„Llama-3.2-1Bæ¨¡å‹"""
    print("ğŸ¤– æ­£åœ¨åŠ è½½TOFUè®­ç»ƒçš„Llama-3.2-1Bæ¨¡å‹...")
    print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {HF_HOME}")
    
    model_cfg = create_model_config()
    
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    model_name = "open-unlearning/tofu_Llama-3.2-1B-Instruct_full"
    cache_path = os.path.join(HF_HOME, "hub", f"models--{model_name.replace('/', '--')}")
    
    if os.path.exists(cache_path):
        print(f"âœ… å‘ç°æœ¬åœ°ç¼“å­˜: {cache_path}")
    else:
        print(f"ğŸ“¥ æœ¬åœ°æ— ç¼“å­˜ï¼Œå°†ä»HuggingFace Hubä¸‹è½½ï¼ˆçº¦16GBï¼‰")
        print(f"ğŸ• è¯·è€å¿ƒç­‰å¾…ä¸‹è½½å®Œæˆ...")
    
    model, tokenizer = get_model(model_cfg)
    print("âœ… å·²è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer

def format_prompt_with_system(question):
    """ä½¿ç”¨ç³»ç»Ÿpromptæ ¼å¼åŒ–é—®é¢˜"""
    system_prompt = "Cutting Knowledge Date: December 2023\nToday Date: 10 Apr 2025\n\nYou are a helpful assistant."
    
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    return prompt

def generate_responses(model, tokenizer, prompt, num_samples=5):
    """ä½¿ç”¨top-Pé‡‡æ ·ç”Ÿæˆå›å¤"""
    print("ğŸ¯ å¼€å§‹ç”Ÿæˆå›å¤...")
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    if hasattr(model, 'device'):
        device = model.device
    else:
        device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    
    print(f"ğŸ”§ è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç”Ÿæˆå‚æ•°ï¼šä½¿ç”¨top-Pé‡‡æ ·
    generation_kwargs = {
        "input_ids": input_ids,
        "do_sample": True,       # å¯ç”¨é‡‡æ ·
        "top_p": 0.9,           # top-På‚æ•°ï¼Œä¿ç•™ç´¯è®¡æ¦‚ç‡å‰90%çš„token
        "temperature": 0.7,      # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
        "max_new_tokens": 200,   # æœ€å¤§ç”Ÿæˆtokenæ•°
        "num_return_sequences": num_samples,  # ç”Ÿæˆ5æ¡å›å¤
        "pad_token_id": tokenizer.eos_token_id,
        "use_cache": True,
        "early_stopping": True
    }
    
    # ç”Ÿæˆå›å¤
    print("âš¡ æ­£åœ¨ç”Ÿæˆ...")
    with torch.no_grad():
        outputs = model.generate(**generation_kwargs)
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    responses = []
    for i in range(num_samples):
        # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹promptï¼‰
        generated_tokens = outputs[i][input_ids.shape[-1]:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append(response.strip())
    
    print("âœ… å›å¤ç”Ÿæˆå®Œæˆ")
    return responses

def generate_responses_by_probability(model, tokenizer, prompt):
    """æ ¹æ®tokenæ¦‚ç‡é‡‡æ ·ä¸åŒç±»å‹çš„å›å¤"""
    print("ğŸ¯ å¼€å§‹ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤...")
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    if hasattr(model, 'device'):
        device = model.device
    else:
        device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    
    print(f"ğŸ”§ è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å®šä¹‰ä¸‰ç§ä¸åŒçš„é‡‡æ ·ç­–ç•¥
    sampling_configs = {
        "é«˜æ¦‚ç‡": {
            "do_sample": True,
            "top_p": 0.3,           # åªè€ƒè™‘ç´¯è®¡æ¦‚ç‡å‰30%çš„tokenï¼ˆæ›´ä¿å®ˆï¼‰
            "temperature": 0.3,      # ä½æ¸©åº¦ï¼Œå€¾å‘äºé«˜æ¦‚ç‡token
            "max_new_tokens": 200,
            "num_return_sequences": 5,
            "pad_token_id": tokenizer.eos_token_id,
            "use_cache": True,
            "early_stopping": True
        },
        "ä¸­æ¦‚ç‡": {
            "do_sample": True,
            "top_p": 0.7,           # è€ƒè™‘ç´¯è®¡æ¦‚ç‡å‰70%çš„token
            "temperature": 0.7,      # ä¸­ç­‰æ¸©åº¦
            "max_new_tokens": 200,
            "num_return_sequences": 5,
            "pad_token_id": tokenizer.eos_token_id,
            "use_cache": True,
            "early_stopping": True
        },
        "ä½æ¦‚ç‡": {
            "do_sample": True,
            "top_p": 0.95,          # è€ƒè™‘æ›´å¤štokenï¼ˆåŒ…æ‹¬ä½æ¦‚ç‡çš„ï¼‰
            "temperature": 1.2,      # é«˜æ¸©åº¦ï¼Œå¢åŠ éšæœºæ€§ï¼Œå€¾å‘äºä½æ¦‚ç‡token
            "max_new_tokens": 200,
            "num_return_sequences": 5,
            "pad_token_id": tokenizer.eos_token_id,
            "use_cache": True,
            "early_stopping": True
        }
    }
    
    all_responses = {}
    
    # ä¸ºæ¯ç§æ¦‚ç‡çº§åˆ«ç”Ÿæˆå›å¤
    for prob_type, config in sampling_configs.items():
        print(f"âš¡ æ­£åœ¨ç”Ÿæˆ{prob_type}å›å¤ (top_p={config['top_p']}, temperature={config['temperature']})...")
        
        generation_kwargs = {
            "input_ids": input_ids,
            **config
        }
        
        with torch.no_grad():
            outputs = model.generate(**generation_kwargs)
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        responses = []
        for i in range(5):
            # åªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹promptï¼‰
            generated_tokens = outputs[i][input_ids.shape[-1]:]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append(response.strip())
        
        all_responses[prob_type] = responses
    
    print("âœ… æ‰€æœ‰æ¦‚ç‡çº§åˆ«çš„å›å¤ç”Ÿæˆå®Œæˆ")
    return all_responses

def generate_responses_by_direct_probability(model, tokenizer, prompt):
    """ç›´æ¥æ ¹æ®tokenæ¦‚ç‡é‡‡æ ·ä¸åŒç±»å‹çš„å›å¤"""
    print("ğŸ¯ å¼€å§‹ç›´æ¥æ¦‚ç‡é‡‡æ ·ç”Ÿæˆå›å¤...")
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    if hasattr(model, 'device'):
        device = model.device
    else:
        device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    
    print(f"ğŸ”§ è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    def sample_from_probability_range(logits, prob_range, num_samples=5):
        """ä»æŒ‡å®šæ¦‚ç‡èŒƒå›´é‡‡æ ·token"""
        # è·å–æ¦‚ç‡åˆ†å¸ƒ
        probs = F.softmax(logits, dim=-1)
        
        # æ ¹æ®æ¦‚ç‡æ’åº
        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        
        # ç¡®å®šæ¦‚ç‡èŒƒå›´çš„ç´¢å¼•åŒºé—´
        vocab_size = probs.shape[-1]
        if prob_range == "é«˜æ¦‚ç‡":
            # å‰10%çš„token
            start_idx = 0
            end_idx = max(1, int(vocab_size * 0.1))
        elif prob_range == "ä¸­æ¦‚ç‡":
            # 10%-50%çš„token
            start_idx = int(vocab_size * 0.1)
            end_idx = int(vocab_size * 0.5)
        else:  # ä½æ¦‚ç‡
            # 50%-90%çš„tokenï¼ˆé¿å…æä½æ¦‚ç‡çš„å™ªå£°tokenï¼‰
            start_idx = int(vocab_size * 0.5)
            end_idx = int(vocab_size * 0.9)
        
        # ä»æŒ‡å®šèŒƒå›´éšæœºé‡‡æ ·
        range_indices = sorted_indices[start_idx:end_idx]
        range_probs = sorted_probs[start_idx:end_idx]
        
        # é‡æ–°å½’ä¸€åŒ–æ¦‚ç‡
        range_probs = range_probs / range_probs.sum()
        
        # è¿›è¡Œé‡‡æ ·
        sampled_tokens = []
        for _ in range(num_samples):
            # æ ¹æ®é‡æ–°å½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·
            sample_idx = torch.multinomial(range_probs, 1)
            sampled_token = range_indices[sample_idx]
            sampled_tokens.append(sampled_token.item())
        
        return sampled_tokens
    
    def generate_sequence(prob_range, max_length=200):
        """ç”ŸæˆæŒ‡å®šæ¦‚ç‡ç±»å‹çš„åºåˆ—"""
        sequences = []
        
        for seq_idx in range(5):  # ç”Ÿæˆ5æ¡å›å¤
            current_ids = input_ids.clone()
            
            for step in range(max_length):
                # å‰å‘ä¼ æ’­è·å–logits
                with torch.no_grad():
                    outputs = model(current_ids)
                    logits = outputs.logits[0, -1, :]  # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                
                # ä»æŒ‡å®šæ¦‚ç‡èŒƒå›´é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                next_tokens = sample_from_probability_range(logits, prob_range, num_samples=1)
                next_token = next_tokens[0]
                
                # æ·»åŠ åˆ°åºåˆ—
                current_ids = torch.cat([current_ids, torch.tensor([[next_token]], device=device)], dim=1)
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸtoken
                if next_token == tokenizer.eos_token_id:
                    break
            
            # è§£ç åºåˆ—ï¼ˆå»æ‰åŸå§‹promptï¼‰
            generated_tokens = current_ids[0, input_ids.shape[1]:]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            sequences.append(response.strip())
        
        return sequences
    
    all_responses = {}
    prob_ranges = ["é«˜æ¦‚ç‡", "ä¸­æ¦‚ç‡", "ä½æ¦‚ç‡"]
    
    # ä¸ºæ¯ç§æ¦‚ç‡çº§åˆ«ç”Ÿæˆå›å¤
    for prob_range in prob_ranges:
        print(f"âš¡ æ­£åœ¨ç”Ÿæˆ{prob_range}å›å¤...")
        if prob_range == "é«˜æ¦‚ç‡":
            print("   ğŸ“Š é‡‡æ ·ç­–ç•¥: ä»å‰10%é«˜æ¦‚ç‡tokenä¸­é€‰æ‹©")
        elif prob_range == "ä¸­æ¦‚ç‡":
            print("   ğŸ“Š é‡‡æ ·ç­–ç•¥: ä»10%-50%ä¸­ç­‰æ¦‚ç‡tokenä¸­é€‰æ‹©")
        else:
            print("   ğŸ“Š é‡‡æ ·ç­–ç•¥: ä»50%-90%ä½æ¦‚ç‡tokenä¸­é€‰æ‹©")
        
        responses = generate_sequence(prob_range)
        all_responses[prob_range] = responses
    
    print("âœ… æ‰€æœ‰æ¦‚ç‡çº§åˆ«çš„å›å¤ç”Ÿæˆå®Œæˆ")
    return all_responses

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ TOFUæ•°æ®é›†ç¬¬18æ¡æ•°æ®ç›´æ¥æ¦‚ç‡é‡‡æ ·ç”Ÿæˆè„šæœ¬å¯åŠ¨")
    print("ğŸ¯ ä½¿ç”¨å·²åœ¨TOFUæ•°æ®é›†ä¸Šè®­ç»ƒçš„Llama-3.2-1Bæ¨¡å‹")
    print("ğŸ² é‡‡æ ·æ–¹å¼: ç›´æ¥ä»é«˜ã€ä¸­ã€ä½æ¦‚ç‡tokenåŒºé—´é‡‡æ ·ï¼Œå„5æ¡")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_tofu_dataset()
    
    # è·å–ç¬¬18æ¡æ•°æ®ï¼ˆç´¢å¼•17ï¼Œå› ä¸ºä»0å¼€å§‹ï¼‰
    target_idx = 17
    if len(dataset) <= target_idx:
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®é›†åªæœ‰{len(dataset)}æ¡æ•°æ®ï¼Œæ— æ³•è·å–ç¬¬18æ¡")
        return
    
    target_data = dataset[target_idx]
    question = target_data["question"]
    answer = target_data["answer"]
    
    print(f"ğŸ“‹ ç¬¬18æ¡æ•°æ®å†…å®¹:")
    print(f"é—®é¢˜: {question}")
    print(f"åŸå§‹ç­”æ¡ˆ: {answer}")
    print("=" * 60)
    
    # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
    model, tokenizer = load_trained_model()
    
    # æ ¼å¼åŒ–prompt
    prompt = format_prompt_with_system(question)
    print(f"ğŸ”§ æ ¼å¼åŒ–åçš„prompté¢„è§ˆ:")
    print(f"{prompt[:200]}...")
    print("=" * 60)
    
    # ç›´æ¥æ¦‚ç‡é‡‡æ ·ç”Ÿæˆå›å¤
    all_responses = generate_responses_by_direct_probability(model, tokenizer, prompt)
    
    # è¾“å‡ºç»“æœ
    print("ğŸ“ TOFUè®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„ç›´æ¥æ¦‚ç‡é‡‡æ ·å›å¤:")
    print("=" * 60)
    
    for prob_type, responses in all_responses.items():
        print(f"ğŸ¯ {prob_type}å›å¤ (å…±5æ¡):")
        if prob_type == "é«˜æ¦‚ç‡":
            print("   ğŸ“Š é‡‡æ ·èŒƒå›´: æ¦‚ç‡æ’åºå‰10%çš„token")
        elif prob_type == "ä¸­æ¦‚ç‡":
            print("   ğŸ“Š é‡‡æ ·èŒƒå›´: æ¦‚ç‡æ’åº10%-50%çš„token")
        else:  # ä½æ¦‚ç‡
            print("   ğŸ“Š é‡‡æ ·èŒƒå›´: æ¦‚ç‡æ’åº50%-90%çš„token")
        
        for i, response in enumerate(responses, 1):
            print(f"   {i}. {response}")
        print("-" * 60)
    
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print("ğŸ’¡ è¯´æ˜ï¼š")
    print("   - é«˜æ¦‚ç‡å›å¤ï¼šæ¯æ­¥éƒ½é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å‰10%token")
    print("   - ä¸­æ¦‚ç‡å›å¤ï¼šæ¯æ­¥éƒ½é€‰æ‹©æ¦‚ç‡ä¸­ç­‰çš„token (10%-50%)")
    print("   - ä½æ¦‚ç‡å›å¤ï¼šæ¯æ­¥éƒ½é€‰æ‹©æ¦‚ç‡è¾ƒä½çš„token (50%-90%)")
    print("   - è¿™ç§æ–¹æ³•èƒ½æ›´ç›´æ¥åœ°æ§åˆ¶ç”Ÿæˆå†…å®¹çš„æ¦‚ç‡ç‰¹æ€§")

if __name__ == "__main__":
    main() 