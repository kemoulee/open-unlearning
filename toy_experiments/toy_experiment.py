#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPO Unlearnå®éªŒè„šæœ¬
ç›‘æµ‹ä¸åŒæ¦‚ç‡å›å¤åœ¨unlearnè¿‡ç¨‹ä¸­çš„log-probabilityå˜åŒ–
"""

import hydra
import torch
import torch.nn.functional as F
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from omegaconf import DictConfig, OmegaConf
from datasets import load_dataset
from transformers import TrainerCallback
import json
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from model import get_model
from data import get_data, get_collators
from trainer import load_trainer
from trainer.utils import seed_everything

class LogProbMonitor(TrainerCallback):
    """ç›‘æµ‹å›å¤log-probabilityçš„å›è°ƒå‡½æ•°"""
    
    def __init__(self, model, tokenizer, responses_dict, template_args):
        self.model = model
        self.tokenizer = tokenizer
        self.responses_dict = responses_dict
        self.template_args = template_args
        self.log_probs_history = []
        self.steps = []
        
    def compute_log_prob(self, question, answer):
        """è®¡ç®—ç»™å®šé—®é¢˜-ç­”æ¡ˆå¯¹çš„log-probability"""
        # æ ¼å¼åŒ–è¾“å…¥ï¼ˆå’Œç”Ÿæˆæ—¶ä¿æŒä¸€è‡´ï¼‰
        system_prompt = "Cutting Knowledge Date: December 2023\nToday Date: 10 Apr 2025\n\nYou are a helpful assistant."
        
        # åˆ†åˆ«ç¼–ç é—®é¢˜å’Œå®Œæ•´prompt
        question_prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        full_prompt = question_prompt + answer
        
        # ç¼–ç 
        question_ids = self.tokenizer.encode(question_prompt, return_tensors="pt", add_special_tokens=False)
        full_ids = self.tokenizer.encode(full_prompt, return_tensors="pt", add_special_tokens=False)
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        device = next(self.model.parameters()).device
        full_ids = full_ids.to(device)
        
        # è®¡ç®—log-probability
        with torch.no_grad():
            outputs = self.model(full_ids)
            logits = outputs.logits[0]  # [seq_len, vocab_size]
            
            # åªè®¡ç®—answeréƒ¨åˆ†çš„log-prob
            answer_start = question_ids.shape[1]
            answer_tokens = full_ids[0, answer_start:]
            answer_logits = logits[answer_start-1:-1]  # shift by 1 for next token prediction
            
            if len(answer_tokens) == 0 or len(answer_logits) == 0:
                return -10.0  # è¿”å›ä¸€ä¸ªé»˜è®¤çš„ä½æ¦‚ç‡å€¼
            
            # è®¡ç®—logæ¦‚ç‡
            log_probs = F.log_softmax(answer_logits, dim=-1)
            token_log_probs = log_probs.gather(1, answer_tokens.unsqueeze(1)).squeeze(1)
            
            # è¿”å›å¹³å‡log-probability
            return token_log_probs.mean().item()
    
    def on_step_end(self, args, state, control, **kwargs):
        """æ¯ä¸ªstepç»“æŸæ—¶è®¡ç®—log-probabilities"""
        if state.global_step % 50 == 0:  # æ¯50æ­¥ç›‘æµ‹ä¸€æ¬¡
            print(f"ğŸ” Step {state.global_step}: ç›‘æµ‹log-probabilities...")
            
            current_log_probs = {}
            
            # è®¡ç®—æ¯ä¸ªå›å¤çš„log-prob
            for response_type, responses in self.responses_dict.items():
                if response_type in ['é«˜æ¦‚ç‡', 'ä¸­æ¦‚ç‡', 'ä½æ¦‚ç‡']:
                    # è®¡ç®—5ä¸ªå›å¤çš„å¹³å‡
                    log_probs = []
                    for response in responses:
                        log_prob = self.compute_log_prob(self.responses_dict['question'], response)
                        log_probs.append(log_prob)
                    current_log_probs[response_type] = np.mean(log_probs)
                else:
                    # ground_truth å’Œ fixed_response
                    log_prob = self.compute_log_prob(self.responses_dict['question'], responses)
                    current_log_probs[response_type] = log_prob
            
            self.log_probs_history.append(current_log_probs)
            self.steps.append(state.global_step)
            
            # è¾“å‡ºå½“å‰ç»“æœ
            print(f"   Ground Truth: {current_log_probs['ground_truth']:.4f}")
            print(f"   Fixed Response: {current_log_probs['fixed_response']:.4f}")
            print(f"   é«˜æ¦‚ç‡å¹³å‡: {current_log_probs['é«˜æ¦‚ç‡']:.4f}")
            print(f"   ä¸­æ¦‚ç‡å¹³å‡: {current_log_probs['ä¸­æ¦‚ç‡']:.4f}")
            print(f"   ä½æ¦‚ç‡å¹³å‡: {current_log_probs['ä½æ¦‚ç‡']:.4f}")

def generate_responses_by_direct_probability(model, tokenizer, question):
    """ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("ğŸ¯ ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤...")
    
    # æ ¼å¼åŒ–prompt
    system_prompt = "Cutting Knowledge Date: December 2023\nToday Date: 10 Apr 2025\n\nYou are a helpful assistant."
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
    
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    
    def sample_from_probability_range(logits, prob_range):
        """ä»æŒ‡å®šæ¦‚ç‡èŒƒå›´é‡‡æ ·token"""
        probs = F.softmax(logits, dim=-1)
        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        
        vocab_size = probs.shape[-1]
        if prob_range == "é«˜æ¦‚ç‡":
            start_idx, end_idx = 0, max(1, int(vocab_size * 0.1))
        elif prob_range == "ä¸­æ¦‚ç‡":
            start_idx, end_idx = int(vocab_size * 0.1), int(vocab_size * 0.5)
        else:  # ä½æ¦‚ç‡
            start_idx, end_idx = int(vocab_size * 0.5), int(vocab_size * 0.9)
        
        range_indices = sorted_indices[start_idx:end_idx]
        range_probs = sorted_probs[start_idx:end_idx]
        range_probs = range_probs / range_probs.sum()
        
        sample_idx = torch.multinomial(range_probs, 1)
        return range_indices[sample_idx].item()
    
    def generate_sequence(prob_range, max_length=50):
        """ç”ŸæˆæŒ‡å®šæ¦‚ç‡ç±»å‹çš„åºåˆ—"""
        sequences = []
        
        for _ in range(5):  # ç”Ÿæˆ5æ¡å›å¤
            current_ids = input_ids.clone()
            
            for _ in range(max_length):
                with torch.no_grad():
                    outputs = model(current_ids)
                    logits = outputs.logits[0, -1, :]
                
                next_token = sample_from_probability_range(logits, prob_range)
                current_ids = torch.cat([current_ids, torch.tensor([[next_token]], device=device)], dim=1)
                
                if next_token == tokenizer.eos_token_id:
                    break
            
            generated_tokens = current_ids[0, input_ids.shape[1]:]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            sequences.append(response.strip())
        
        return sequences
    
    # ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤
    responses = {}
    for prob_range in ["é«˜æ¦‚ç‡", "ä¸­æ¦‚ç‡", "ä½æ¦‚ç‡"]:
        print(f"   ç”Ÿæˆ{prob_range}å›å¤...")
        responses[prob_range] = generate_sequence(prob_range)
    
    return responses

def plot_log_probs(steps, log_probs_history, save_path):
    """ç»˜åˆ¶log-probabilityå˜åŒ–å›¾"""
    print("ğŸ“Š ç»˜åˆ¶log-probabilityå˜åŒ–å›¾...")
    
    plt.figure(figsize=(12, 8))
    
    # æå–æ¯æ¡çº¿çš„æ•°æ®
    ground_truth_probs = [log_probs['ground_truth'] for log_probs in log_probs_history]
    fixed_response_probs = [log_probs['fixed_response'] for log_probs in log_probs_history]
    high_prob_probs = [log_probs['é«˜æ¦‚ç‡'] for log_probs in log_probs_history]
    mid_prob_probs = [log_probs['ä¸­æ¦‚ç‡'] for log_probs in log_probs_history]
    low_prob_probs = [log_probs['ä½æ¦‚ç‡'] for log_probs in log_probs_history]
    
    # ç»˜åˆ¶æ›²çº¿
    plt.plot(steps, ground_truth_probs, 'g-', linewidth=2, label='Ground Truth', marker='o')
    plt.plot(steps, fixed_response_probs, 'r-', linewidth=2, label='Fixed Response ("She mainly writes in English.")', marker='s')
    plt.plot(steps, high_prob_probs, 'b-', linewidth=2, label='é«˜æ¦‚ç‡å›å¤å¹³å‡', marker='^')
    plt.plot(steps, mid_prob_probs, 'orange', linewidth=2, label='ä¸­æ¦‚ç‡å›å¤å¹³å‡', marker='v')
    plt.plot(steps, low_prob_probs, 'purple', linewidth=2, label='ä½æ¦‚ç‡å›å¤å¹³å‡', marker='d')
    
    plt.xlabel('Training Steps', fontsize=12)
    plt.ylabel('Log-Probability', fontsize=12)
    plt.title('NPO Unlearningè¿‡ç¨‹ä¸­å›å¤Log-Probabilityå˜åŒ–', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")

@hydra.main(version_base=None, config_path=".", config_name="experiment_config.yaml")
def main(cfg: DictConfig):
    """ä¸»å®éªŒå‡½æ•°"""
    print("ğŸš€ NPO Unlearnå®éªŒå¼€å§‹")
    print("=" * 60)
    
    print(f"ğŸ”§ å®éªŒé…ç½®:")
    print(f"   Trainer: {cfg.trainer.handler}")
    print(f"   Model: {cfg.model.model_args.pretrained_model_name_or_path}")
    print(f"   Forget Split: {cfg.forget_split}")
    print(f"   Retain Split: {cfg.retain_split}")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(42)
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    model, tokenizer = get_model(cfg.model)
    
    # åŠ è½½TOFUæ•°æ®é›†ç¬¬18æ¡æ•°æ®
    print("ğŸ“¥ åŠ è½½TOFUæ•°æ®é›†...")
    dataset = load_dataset("locuslab/TOFU", name="forget10", split="train")
    target_idx = 17  # ç¬¬18æ¡æ•°æ®
    target_data = dataset[target_idx]
    question = target_data["question"]
    ground_truth = target_data["answer"]
    
    print(f"ğŸ“‹ å®éªŒæ•°æ®:")
    print(f"   é—®é¢˜: {question}")
    print(f"   Ground Truth: {ground_truth}")
    print("=" * 60)
    
    # ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤
    print("ğŸ² ç”Ÿæˆä¸åŒæ¦‚ç‡çº§åˆ«çš„å›å¤...")
    generated_responses = generate_responses_by_direct_probability(model, tokenizer, question)
    
    # å‡†å¤‡ç›‘æµ‹çš„å›å¤å­—å…¸
    responses_dict = {
        'question': question,
        'ground_truth': ground_truth,
        'fixed_response': "She mainly writes in English.",
        **generated_responses
    }
    
    print("ğŸ“ ç”Ÿæˆçš„å›å¤:")
    for response_type, responses in generated_responses.items():
        print(f"   {response_type}: {len(responses)}æ¡")
        for i, response in enumerate(responses[:2], 1):  # åªæ˜¾ç¤ºå‰2æ¡
            print(f"     {i}. {response[:50]}...")
    print("=" * 60)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    data = get_data(cfg.data, mode="unlearn", tokenizer=tokenizer, template_args=cfg.model.template_args)
    collator = get_collators(cfg.collator, tokenizer=tokenizer)
    
    # åˆ›å»ºlog-probç›‘æµ‹å™¨
    monitor = LogProbMonitor(model, tokenizer, responses_dict, cfg.model.template_args)
    
    # è®¾ç½®trainer
    print("ğŸƒ è®¾ç½®trainer...")
    trainer, trainer_args = load_trainer(
        trainer_cfg=cfg.trainer,
        model=model,
        train_dataset=data.get("train", None),
        eval_dataset=data.get("eval", None),
        tokenizer=tokenizer,
        data_collator=collator,
        evaluators=None,
        template_args=cfg.model.template_args,
    )
    
    # æ·»åŠ å›è°ƒå‡½æ•°
    trainer.add_callback(monitor)
    
    # æ‰§è¡Œåˆå§‹ç›‘æµ‹
    print("ğŸ” æ‰§è¡Œåˆå§‹ç›‘æµ‹...")
    monitor.on_step_end(trainer_args, trainer.state, None)
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ‹ï¸ å¼€å§‹NPO unlearning...")
    trainer.train()
    
    # ç»˜åˆ¶ç»“æœ
    save_path = "toy_experiments/npo_unlearn_log_probs.png"
    plot_log_probs(monitor.steps, monitor.log_probs_history, save_path)
    
    # ä¿å­˜æ•°æ®
    results = {
        'steps': monitor.steps,
        'log_probs_history': monitor.log_probs_history,
        'question': question,
        'ground_truth': ground_truth,
        'generated_responses': generated_responses
    }
    
    results_path = "toy_experiments/npo_unlearn_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("ğŸ‰ å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ˆ å›¾è¡¨ä¿å­˜åˆ°: {save_path}")
    print(f"ğŸ’¾ æ•°æ®ä¿å­˜åˆ°: {results_path}")

if __name__ == "__main__":
    main() 